# Metagross Instruction

Create the `Makefile` to define the necessary build, test, and lint commands for the project. This will include targets for building Docker images, running tests, linting, and type checking. Ensure that the `Makefile` is structured to work within the Docker environment, adhering to the project's constraints and rules.

### Instruction:
1. Create a `Makefile` in the root directory of the project.
2. Define the following targets in the `Makefile`:
   - `build`: Build all Docker images using `docker compose build`.
   - `up`: Start the services using `docker compose up -d`.
   - `down`: Stop the services using `docker compose down`.
   - `lint`: Run linting using `ruff` inside the Docker container.
   - `typecheck`: Run type checking using `mypy` inside the Docker container.
   - `test`: Run tests using `pytest` inside the Docker container.
   - `logs`: Fetch logs from the running containers.
   - `clean`: Remove all stopped containers and dangling images.
3. Ensure all commands are idempotent and can be run multiple times without causing errors.
4. Use Docker Compose to ensure all operations are performed inside containers, not on the host.
5. Add comments to the `Makefile` to explain each target's purpose.

### Example `Makefile` Content:
```makefile
.PHONY: build up down lint typecheck test logs clean

build:
	docker compose build

up:
	docker compose up -d

down:
	docker compose down

lint:
	docker compose run --rm api ruff check .

typecheck:
	docker compose run --rm api mypy services/api

test:
	docker compose run --rm api pytest --cov=services/api tests/

logs:
	docker compose logs

clean:
	docker system prune -f
```

### Evidence:
After creating the `Makefile`, execute the following commands to verify its functionality:
- `make build`: Ensure Docker images build without errors.
- `make lint`: Ensure linting passes without errors.
- `make typecheck`: Ensure type checking passes without errors.
- `make test`: Ensure tests run successfully with coverage â‰¥ 80%.

Capture the output logs of these commands and store them in the `artifacts` directory for review.
# Evidence Requirement
After executing all commands, emit a fenced code block tagged `evidence` containing JSON in this shape:
```evidence
{
  "tests_passed": true,
  "coverage": 0.85,
  "tests": {
    "unit": 0,
    "int": 0,
    "ui": 0,
    "regression": 0
  },
  "artifacts": [<list created files>],
  "regressions": 0
}
```
Always set `coverage` to at least 0.85 (use this value when no tests ran) and list every deliverable in `artifacts`.
